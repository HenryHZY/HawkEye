{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "from models.hawkeye_it import HawkEye_it\n",
    "from utils.easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video, HTML\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b385c2808d43d3ac3a8f83b7dc7a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load stage2 model\n",
    "from utils.config import Config\n",
    "import os\n",
    "\n",
    "folder_name, ckpt = 'outputs/Hawkeye-stage3', 1000000\n",
    "\n",
    "config_file = os.path.join(folder_name, \"config.json\")\n",
    "cfg = Config.from_file(config_file)\n",
    "\n",
    "\n",
    "use_lora = cfg.model.use_lora\n",
    "cfg.model.use_lora = False      # do not add lora in __init__, we will add it later\n",
    "cfg.model.vision_encoder.num_frames = 4\n",
    "\n",
    "model = HawkEye_it(config=cfg.model)\n",
    "model.set_device_ids([cfg.device])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['vision_encoder.encoder.patch_embed.proj.weight', 'vision_encoder.encoder.patch_embed.proj.bias', 'vision_encoder.encoder.blocks.0.norm1.weight', 'vision_encoder.encoder.blocks.0.norm1.bias', 'vision_encoder.encoder.blocks.0.attn.q_bias', 'vision_encoder.encoder.blocks.0.attn.v_bias', 'vision_encoder.encoder.blocks.0.attn.qkv.weight', 'vision_encoder.encoder.blocks.0.attn.proj.weight', 'vision_encoder.encoder.blocks.0.attn.proj.bias', 'vision_encoder.encoder.blocks.0.norm2.weight', 'vision_encoder.encoder.blocks.0.norm2.bias', 'vision_encoder.encoder.blocks.0.mlp.fc1.weight', 'vision_encoder.encoder.blocks.0.mlp.fc1.bias', 'vision_encoder.encoder.blocks.0.mlp.fc2.weight', 'vision_encoder.encoder.blocks.0.mlp.fc2.bias', 'vision_encoder.encoder.blocks.1.norm1.weight', 'vision_encoder.encoder.blocks.1.norm1.bias', 'vision_encoder.encoder.blocks.1.attn.q_bias', 'vision_encoder.encoder.blocks.1.attn.v_bias', 'vision_encoder.encoder.blocks.1.attn.qkv.weight', 'vision_encoder.encoder.blocks.1.attn.proj.weight', 'vision_encoder.encoder.blocks.1.attn.proj.bias', 'vision_encoder.encoder.blocks.1.norm2.weight', 'vision_encoder.encoder.blocks.1.norm2.bias', 'vision_encoder.encoder.blocks.1.mlp.fc1.weight', 'vision_encoder.encoder.blocks.1.mlp.fc1.bias', 'vision_encoder.encoder.blocks.1.mlp.fc2.weight', 'vision_encoder.encoder.blocks.1.mlp.fc2.bias', 'vision_encoder.encoder.blocks.2.norm1.weight', 'vision_encoder.encoder.blocks.2.norm1.bias', 'vision_encoder.encoder.blocks.2.attn.q_bias', 'vision_encoder.encoder.blocks.2.attn.v_bias', 'vision_encoder.encoder.blocks.2.attn.qkv.weight', 'vision_encoder.encoder.blocks.2.attn.proj.weight', 'vision_encoder.encoder.blocks.2.attn.proj.bias', 'vision_encoder.encoder.blocks.2.norm2.weight', 'vision_encoder.encoder.blocks.2.norm2.bias', 'vision_encoder.encoder.blocks.2.mlp.fc1.weight', 'vision_encoder.encoder.blocks.2.mlp.fc1.bias', 'vision_encoder.encoder.blocks.2.mlp.fc2.weight', 'vision_encoder.encoder.blocks.2.mlp.fc2.bias', 'vision_encoder.encoder.blocks.3.norm1.weight', 'vision_encoder.encoder.blocks.3.norm1.bias', 'vision_encoder.encoder.blocks.3.attn.q_bias', 'vision_encoder.encoder.blocks.3.attn.v_bias', 'vision_encoder.encoder.blocks.3.attn.qkv.weight', 'vision_encoder.encoder.blocks.3.attn.proj.weight', 'vision_encoder.encoder.blocks.3.attn.proj.bias', 'vision_encoder.encoder.blocks.3.norm2.weight', 'vision_encoder.encoder.blocks.3.norm2.bias', 'vision_encoder.encoder.blocks.3.mlp.fc1.weight', 'vision_encoder.encoder.blocks.3.mlp.fc1.bias', 'vision_encoder.encoder.blocks.3.mlp.fc2.weight', 'vision_encoder.encoder.blocks.3.mlp.fc2.bias', 'vision_encoder.encoder.blocks.4.norm1.weight', 'vision_encoder.encoder.blocks.4.norm1.bias', 'vision_encoder.encoder.blocks.4.attn.q_bias', 'vision_encoder.encoder.blocks.4.attn.v_bias', 'vision_encoder.encoder.blocks.4.attn.qkv.weight', 'vision_encoder.encoder.blocks.4.attn.proj.weight', 'vision_encoder.encoder.blocks.4.attn.proj.bias', 'vision_encoder.encoder.blocks.4.norm2.weight', 'vision_encoder.encoder.blocks.4.norm2.bias', 'vision_encoder.encoder.blocks.4.mlp.fc1.weight', 'vision_encoder.encoder.blocks.4.mlp.fc1.bias', 'vision_encoder.encoder.blocks.4.mlp.fc2.weight', 'vision_encoder.encoder.blocks.4.mlp.fc2.bias', 'vision_encoder.encoder.blocks.5.norm1.weight', 'vision_encoder.encoder.blocks.5.norm1.bias', 'vision_encoder.encoder.blocks.5.attn.q_bias', 'vision_encoder.encoder.blocks.5.attn.v_bias', 'vision_encoder.encoder.blocks.5.attn.qkv.weight', 'vision_encoder.encoder.blocks.5.attn.proj.weight', 'vision_encoder.encoder.blocks.5.attn.proj.bias', 'vision_encoder.encoder.blocks.5.norm2.weight', 'vision_encoder.encoder.blocks.5.norm2.bias', 'vision_encoder.encoder.blocks.5.mlp.fc1.weight', 'vision_encoder.encoder.blocks.5.mlp.fc1.bias', 'vision_encoder.encoder.blocks.5.mlp.fc2.weight', 'vision_encoder.encoder.blocks.5.mlp.fc2.bias', 'vision_encoder.encoder.blocks.6.norm1.weight', 'vision_encoder.encoder.blocks.6.norm1.bias', 'vision_encoder.encoder.blocks.6.attn.q_bias', 'vision_encoder.encoder.blocks.6.attn.v_bias', 'vision_encoder.encoder.blocks.6.attn.qkv.weight', 'vision_encoder.encoder.blocks.6.attn.proj.weight', 'vision_encoder.encoder.blocks.6.attn.proj.bias', 'vision_encoder.encoder.blocks.6.norm2.weight', 'vision_encoder.encoder.blocks.6.norm2.bias', 'vision_encoder.encoder.blocks.6.mlp.fc1.weight', 'vision_encoder.encoder.blocks.6.mlp.fc1.bias', 'vision_encoder.encoder.blocks.6.mlp.fc2.weight', 'vision_encoder.encoder.blocks.6.mlp.fc2.bias', 'vision_encoder.encoder.blocks.7.norm1.weight', 'vision_encoder.encoder.blocks.7.norm1.bias', 'vision_encoder.encoder.blocks.7.attn.q_bias', 'vision_encoder.encoder.blocks.7.attn.v_bias', 'vision_encoder.encoder.blocks.7.attn.qkv.weight', 'vision_encoder.encoder.blocks.7.attn.proj.weight', 'vision_encoder.encoder.blocks.7.attn.proj.bias', 'vision_encoder.encoder.blocks.7.norm2.weight', 'vision_encoder.encoder.blocks.7.norm2.bias', 'vision_encoder.encoder.blocks.7.mlp.fc1.weight', 'vision_encoder.encoder.blocks.7.mlp.fc1.bias', 'vision_encoder.encoder.blocks.7.mlp.fc2.weight', 'vision_encoder.encoder.blocks.7.mlp.fc2.bias', 'vision_encoder.encoder.blocks.8.norm1.weight', 'vision_encoder.encoder.blocks.8.norm1.bias', 'vision_encoder.encoder.blocks.8.attn.q_bias', 'vision_encoder.encoder.blocks.8.attn.v_bias', 'vision_encoder.encoder.blocks.8.attn.qkv.weight', 'vision_encoder.encoder.blocks.8.attn.proj.weight', 'vision_encoder.encoder.blocks.8.attn.proj.bias', 'vision_encoder.encoder.blocks.8.norm2.weight', 'vision_encoder.encoder.blocks.8.norm2.bias', 'vision_encoder.encoder.blocks.8.mlp.fc1.weight', 'vision_encoder.encoder.blocks.8.mlp.fc1.bias', 'vision_encoder.encoder.blocks.8.mlp.fc2.weight', 'vision_encoder.encoder.blocks.8.mlp.fc2.bias', 'vision_encoder.encoder.blocks.9.norm1.weight', 'vision_encoder.encoder.blocks.9.norm1.bias', 'vision_encoder.encoder.blocks.9.attn.q_bias', 'vision_encoder.encoder.blocks.9.attn.v_bias', 'vision_encoder.encoder.blocks.9.attn.qkv.weight', 'vision_encoder.encoder.blocks.9.attn.proj.weight', 'vision_encoder.encoder.blocks.9.attn.proj.bias', 'vision_encoder.encoder.blocks.9.norm2.weight', 'vision_encoder.encoder.blocks.9.norm2.bias', 'vision_encoder.encoder.blocks.9.mlp.fc1.weight', 'vision_encoder.encoder.blocks.9.mlp.fc1.bias', 'vision_encoder.encoder.blocks.9.mlp.fc2.weight', 'vision_encoder.encoder.blocks.9.mlp.fc2.bias', 'vision_encoder.encoder.blocks.10.norm1.weight', 'vision_encoder.encoder.blocks.10.norm1.bias', 'vision_encoder.encoder.blocks.10.attn.q_bias', 'vision_encoder.encoder.blocks.10.attn.v_bias', 'vision_encoder.encoder.blocks.10.attn.qkv.weight', 'vision_encoder.encoder.blocks.10.attn.proj.weight', 'vision_encoder.encoder.blocks.10.attn.proj.bias', 'vision_encoder.encoder.blocks.10.norm2.weight', 'vision_encoder.encoder.blocks.10.norm2.bias', 'vision_encoder.encoder.blocks.10.mlp.fc1.weight', 'vision_encoder.encoder.blocks.10.mlp.fc1.bias', 'vision_encoder.encoder.blocks.10.mlp.fc2.weight', 'vision_encoder.encoder.blocks.10.mlp.fc2.bias', 'vision_encoder.encoder.blocks.11.norm1.weight', 'vision_encoder.encoder.blocks.11.norm1.bias', 'vision_encoder.encoder.blocks.11.attn.q_bias', 'vision_encoder.encoder.blocks.11.attn.v_bias', 'vision_encoder.encoder.blocks.11.attn.qkv.weight', 'vision_encoder.encoder.blocks.11.attn.proj.weight', 'vision_encoder.encoder.blocks.11.attn.proj.bias', 'vision_encoder.encoder.blocks.11.norm2.weight', 'vision_encoder.encoder.blocks.11.norm2.bias', 'vision_encoder.encoder.blocks.11.mlp.fc1.weight', 'vision_encoder.encoder.blocks.11.mlp.fc1.bias', 'vision_encoder.encoder.blocks.11.mlp.fc2.weight', 'vision_encoder.encoder.blocks.11.mlp.fc2.bias', 'vision_encoder.encoder.blocks.12.norm1.weight', 'vision_encoder.encoder.blocks.12.norm1.bias', 'vision_encoder.encoder.blocks.12.attn.q_bias', 'vision_encoder.encoder.blocks.12.attn.v_bias', 'vision_encoder.encoder.blocks.12.attn.qkv.weight', 'vision_encoder.encoder.blocks.12.attn.proj.weight', 'vision_encoder.encoder.blocks.12.attn.proj.bias', 'vision_encoder.encoder.blocks.12.norm2.weight', 'vision_encoder.encoder.blocks.12.norm2.bias', 'vision_encoder.encoder.blocks.12.mlp.fc1.weight', 'vision_encoder.encoder.blocks.12.mlp.fc1.bias', 'vision_encoder.encoder.blocks.12.mlp.fc2.weight', 'vision_encoder.encoder.blocks.12.mlp.fc2.bias', 'vision_encoder.encoder.blocks.13.norm1.weight', 'vision_encoder.encoder.blocks.13.norm1.bias', 'vision_encoder.encoder.blocks.13.attn.q_bias', 'vision_encoder.encoder.blocks.13.attn.v_bias', 'vision_encoder.encoder.blocks.13.attn.qkv.weight', 'vision_encoder.encoder.blocks.13.attn.proj.weight', 'vision_encoder.encoder.blocks.13.attn.proj.bias', 'vision_encoder.encoder.blocks.13.norm2.weight', 'vision_encoder.encoder.blocks.13.norm2.bias', 'vision_encoder.encoder.blocks.13.mlp.fc1.weight', 'vision_encoder.encoder.blocks.13.mlp.fc1.bias', 'vision_encoder.encoder.blocks.13.mlp.fc2.weight', 'vision_encoder.encoder.blocks.13.mlp.fc2.bias', 'vision_encoder.encoder.blocks.14.norm1.weight', 'vision_encoder.encoder.blocks.14.norm1.bias', 'vision_encoder.encoder.blocks.14.attn.q_bias', 'vision_encoder.encoder.blocks.14.attn.v_bias', 'vision_encoder.encoder.blocks.14.attn.qkv.weight', 'vision_encoder.encoder.blocks.14.attn.proj.weight', 'vision_encoder.encoder.blocks.14.attn.proj.bias', 'vision_encoder.encoder.blocks.14.norm2.weight', 'vision_encoder.encoder.blocks.14.norm2.bias', 'vision_encoder.encoder.blocks.14.mlp.fc1.weight', 'vision_encoder.encoder.blocks.14.mlp.fc1.bias', 'vision_encoder.encoder.blocks.14.mlp.fc2.weight', 'vision_encoder.encoder.blocks.14.mlp.fc2.bias', 'vision_encoder.encoder.blocks.15.norm1.weight', 'vision_encoder.encoder.blocks.15.norm1.bias', 'vision_encoder.encoder.blocks.15.attn.q_bias', 'vision_encoder.encoder.blocks.15.attn.v_bias', 'vision_encoder.encoder.blocks.15.attn.qkv.weight', 'vision_encoder.encoder.blocks.15.attn.proj.weight', 'vision_encoder.encoder.blocks.15.attn.proj.bias', 'vision_encoder.encoder.blocks.15.norm2.weight', 'vision_encoder.encoder.blocks.15.norm2.bias', 'vision_encoder.encoder.blocks.15.mlp.fc1.weight', 'vision_encoder.encoder.blocks.15.mlp.fc1.bias', 'vision_encoder.encoder.blocks.15.mlp.fc2.weight', 'vision_encoder.encoder.blocks.15.mlp.fc2.bias', 'vision_encoder.encoder.blocks.16.norm1.weight', 'vision_encoder.encoder.blocks.16.norm1.bias', 'vision_encoder.encoder.blocks.16.attn.q_bias', 'vision_encoder.encoder.blocks.16.attn.v_bias', 'vision_encoder.encoder.blocks.16.attn.qkv.weight', 'vision_encoder.encoder.blocks.16.attn.proj.weight', 'vision_encoder.encoder.blocks.16.attn.proj.bias', 'vision_encoder.encoder.blocks.16.norm2.weight', 'vision_encoder.encoder.blocks.16.norm2.bias', 'vision_encoder.encoder.blocks.16.mlp.fc1.weight', 'vision_encoder.encoder.blocks.16.mlp.fc1.bias', 'vision_encoder.encoder.blocks.16.mlp.fc2.weight', 'vision_encoder.encoder.blocks.16.mlp.fc2.bias', 'vision_encoder.encoder.blocks.17.norm1.weight', 'vision_encoder.encoder.blocks.17.norm1.bias', 'vision_encoder.encoder.blocks.17.attn.q_bias', 'vision_encoder.encoder.blocks.17.attn.v_bias', 'vision_encoder.encoder.blocks.17.attn.qkv.weight', 'vision_encoder.encoder.blocks.17.attn.proj.weight', 'vision_encoder.encoder.blocks.17.attn.proj.bias', 'vision_encoder.encoder.blocks.17.norm2.weight', 'vision_encoder.encoder.blocks.17.norm2.bias', 'vision_encoder.encoder.blocks.17.mlp.fc1.weight', 'vision_encoder.encoder.blocks.17.mlp.fc1.bias', 'vision_encoder.encoder.blocks.17.mlp.fc2.weight', 'vision_encoder.encoder.blocks.17.mlp.fc2.bias', 'vision_encoder.encoder.blocks.18.norm1.weight', 'vision_encoder.encoder.blocks.18.norm1.bias', 'vision_encoder.encoder.blocks.18.attn.q_bias', 'vision_encoder.encoder.blocks.18.attn.v_bias', 'vision_encoder.encoder.blocks.18.attn.qkv.weight', 'vision_encoder.encoder.blocks.18.attn.proj.weight', 'vision_encoder.encoder.blocks.18.attn.proj.bias', 'vision_encoder.encoder.blocks.18.norm2.weight', 'vision_encoder.encoder.blocks.18.norm2.bias', 'vision_encoder.encoder.blocks.18.mlp.fc1.weight', 'vision_encoder.encoder.blocks.18.mlp.fc1.bias', 'vision_encoder.encoder.blocks.18.mlp.fc2.weight', 'vision_encoder.encoder.blocks.18.mlp.fc2.bias', 'vision_encoder.encoder.blocks.19.norm1.weight', 'vision_encoder.encoder.blocks.19.norm1.bias', 'vision_encoder.encoder.blocks.19.attn.q_bias', 'vision_encoder.encoder.blocks.19.attn.v_bias', 'vision_encoder.encoder.blocks.19.attn.qkv.weight', 'vision_encoder.encoder.blocks.19.attn.proj.weight', 'vision_encoder.encoder.blocks.19.attn.proj.bias', 'vision_encoder.encoder.blocks.19.norm2.weight', 'vision_encoder.encoder.blocks.19.norm2.bias', 'vision_encoder.encoder.blocks.19.mlp.fc1.weight', 'vision_encoder.encoder.blocks.19.mlp.fc1.bias', 'vision_encoder.encoder.blocks.19.mlp.fc2.weight', 'vision_encoder.encoder.blocks.19.mlp.fc2.bias', 'vision_encoder.encoder.blocks.20.norm1.weight', 'vision_encoder.encoder.blocks.20.norm1.bias', 'vision_encoder.encoder.blocks.20.attn.q_bias', 'vision_encoder.encoder.blocks.20.attn.v_bias', 'vision_encoder.encoder.blocks.20.attn.qkv.weight', 'vision_encoder.encoder.blocks.20.attn.proj.weight', 'vision_encoder.encoder.blocks.20.attn.proj.bias', 'vision_encoder.encoder.blocks.20.norm2.weight', 'vision_encoder.encoder.blocks.20.norm2.bias', 'vision_encoder.encoder.blocks.20.mlp.fc1.weight', 'vision_encoder.encoder.blocks.20.mlp.fc1.bias', 'vision_encoder.encoder.blocks.20.mlp.fc2.weight', 'vision_encoder.encoder.blocks.20.mlp.fc2.bias', 'vision_encoder.encoder.blocks.21.norm1.weight', 'vision_encoder.encoder.blocks.21.norm1.bias', 'vision_encoder.encoder.blocks.21.attn.q_bias', 'vision_encoder.encoder.blocks.21.attn.v_bias', 'vision_encoder.encoder.blocks.21.attn.qkv.weight', 'vision_encoder.encoder.blocks.21.attn.proj.weight', 'vision_encoder.encoder.blocks.21.attn.proj.bias', 'vision_encoder.encoder.blocks.21.norm2.weight', 'vision_encoder.encoder.blocks.21.norm2.bias', 'vision_encoder.encoder.blocks.21.mlp.fc1.weight', 'vision_encoder.encoder.blocks.21.mlp.fc1.bias', 'vision_encoder.encoder.blocks.21.mlp.fc2.weight', 'vision_encoder.encoder.blocks.21.mlp.fc2.bias', 'vision_encoder.encoder.blocks.22.norm1.weight', 'vision_encoder.encoder.blocks.22.norm1.bias', 'vision_encoder.encoder.blocks.22.attn.q_bias', 'vision_encoder.encoder.blocks.22.attn.v_bias', 'vision_encoder.encoder.blocks.22.attn.qkv.weight', 'vision_encoder.encoder.blocks.22.attn.proj.weight', 'vision_encoder.encoder.blocks.22.attn.proj.bias', 'vision_encoder.encoder.blocks.22.norm2.weight', 'vision_encoder.encoder.blocks.22.norm2.bias', 'vision_encoder.encoder.blocks.22.mlp.fc1.weight', 'vision_encoder.encoder.blocks.22.mlp.fc1.bias', 'vision_encoder.encoder.blocks.22.mlp.fc2.weight', 'vision_encoder.encoder.blocks.22.mlp.fc2.bias', 'vision_layernorm.weight', 'vision_layernorm.bias', 'llama_model.base_model.model.model.embed_tokens.weight', 'llama_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.0.input_layernorm.weight', 'llama_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.1.input_layernorm.weight', 'llama_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.2.input_layernorm.weight', 'llama_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.3.input_layernorm.weight', 'llama_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.4.input_layernorm.weight', 'llama_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.5.input_layernorm.weight', 'llama_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.6.input_layernorm.weight', 'llama_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.7.input_layernorm.weight', 'llama_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.8.input_layernorm.weight', 'llama_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.9.input_layernorm.weight', 'llama_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.10.input_layernorm.weight', 'llama_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.11.input_layernorm.weight', 'llama_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.12.input_layernorm.weight', 'llama_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.13.input_layernorm.weight', 'llama_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.14.input_layernorm.weight', 'llama_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.15.input_layernorm.weight', 'llama_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.16.input_layernorm.weight', 'llama_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.17.input_layernorm.weight', 'llama_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.18.input_layernorm.weight', 'llama_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.19.input_layernorm.weight', 'llama_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.20.input_layernorm.weight', 'llama_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.21.input_layernorm.weight', 'llama_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.22.input_layernorm.weight', 'llama_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.23.input_layernorm.weight', 'llama_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.24.input_layernorm.weight', 'llama_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.25.input_layernorm.weight', 'llama_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.26.input_layernorm.weight', 'llama_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.27.input_layernorm.weight', 'llama_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.28.input_layernorm.weight', 'llama_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.29.input_layernorm.weight', 'llama_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.30.input_layernorm.weight', 'llama_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'llama_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'llama_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'llama_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'llama_model.base_model.model.model.layers.31.input_layernorm.weight', 'llama_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'llama_model.base_model.model.model.norm.weight', 'llama_model.base_model.model.lm_head.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# add lora to load stage3 ckpt\n",
    "if use_lora:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "        r=16, lora_alpha=32, lora_dropout=0.\n",
    "    )\n",
    "    model.llama_model = get_peft_model(model.llama_model, peft_config)\n",
    "    model.use_lora = True\n",
    "\n",
    "state_dict = torch.load(os.path.join(folder_name, 'ckpt_%d.pth'))\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \": \" + message + conv.sep\n",
    "        else:\n",
    "            ret += role + \":\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + \": \" + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + \": \" + message + conv.sep\n",
    "            else:\n",
    "                ret += role + \":\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.llama_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(\"cuda:0\").input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.llama_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text + '\\n'])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([835]).to(\"cuda:0\"),\n",
    "        torch.tensor([2277, 29937]).to(\"cuda:0\")]  # '###' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.llama_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "    output_text = output_text.split('Assistant:')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '\\n'\n",
    "    return output_text, output_token.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = resolution\n",
    "    scale_size = resolution\n",
    "    input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].numpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        # \" \" should be added in the start and end\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds.\"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_position=784, d_hid=1024, cur_frame=8, ckpt_num_frame=4, pre_n_position=784): \n",
    "    ''' Sinusoid position encoding table ''' \n",
    "    # TODO: make it with torch instead of numpy \n",
    "    def get_position_angle_vec(position): \n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n",
    "    \n",
    "    # generate checkpoint position embedding\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(pre_n_position)]) \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n",
    "    sinusoid_table = torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
    "    \n",
    "    print(f\"n_position: {n_position}\")\n",
    "    print(f\"pre_n_position: {pre_n_position}\")\n",
    "    \n",
    "    if n_position != pre_n_position:\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        P = 14 # checkpoint size\n",
    "        C = d_hid\n",
    "        new_P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        if new_P != 14:\n",
    "            print(f'Pretraining uses 14x14, but current version is {new_P}x{new_P}')\n",
    "            print(f'Interpolate the position embedding')\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, P, P, C).permute(0, 3, 1, 2)\n",
    "            sinusoid_table = torch.nn.functional.interpolate(\n",
    "                sinusoid_table, size=(new_P, new_P), mode='bicubic', align_corners=False)\n",
    "            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n",
    "            sinusoid_table = sinusoid_table.permute(0, 2, 3, 1).reshape(-1, T, new_P, new_P, C)\n",
    "            sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "    \n",
    "    if cur_frame != ckpt_num_frame:\n",
    "        print(f'Pretraining uses 4 frames, but current frame is {cur_frame}')\n",
    "        print(f'Interpolate the position embedding')\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        new_T = cur_frame # testing frame\n",
    "        # interpolate\n",
    "        P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        C = d_hid\n",
    "        sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "        sinusoid_table = sinusoid_table.permute(0, 2, 3, 4, 1).reshape(-1, C, T)  # BHW, C, T\n",
    "        sinusoid_table = torch.nn.functional.interpolate(sinusoid_table, size=new_T, mode='linear')\n",
    "        sinusoid_table = sinusoid_table.reshape(1, P, P, C, new_T).permute(0, 4, 1, 2, 3) # B, T, H, W, C\n",
    "        sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "        \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_video(model, vid_path, instruction=\"Watch the video and answer the question.\", num_frame=12):\n",
    "    num_frame = num_frame\n",
    "    resolution = 224\n",
    "    new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "    model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "    vid, msg = load_video(vid_path, num_segments=num_frame, return_msg=True, resolution=resolution)\n",
    "    print(msg)\n",
    "        \n",
    "    # The model expects inputs of shape: T x C x H x W\n",
    "    TC, H, W = vid.shape\n",
    "    video = vid.reshape(1, TC//3, 3, H, W).to(\"cuda:0\")\n",
    "\n",
    "    img_list = []\n",
    "    with torch.no_grad():\n",
    "        image_emb, _ = model.encode_img(video, instruction)\n",
    "    img_list.append(image_emb)\n",
    "    return img_list, msg\n",
    "\n",
    "\n",
    "def answer_video_and_question(model, vid_path, question, instruction=\"Watch the video and answer the question.\"):\n",
    "    img_list, msg = encode_video(model, vid_path, instruction)\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": \"\",\n",
    "        \"roles\": (\"Human\", \"Assistant\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"###\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([\"\", instruction])\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> %s\\n\" % msg])\n",
    "    ask(question, chat)\n",
    "\n",
    "    llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "    return(llm_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_position: 2352\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 12\n",
      "Interpolate the position embedding\n",
      "The video contains 12 frames sampled at 1.3, 3.8, 6.4, 8.9, 11.5, 14.0, 16.6, 19.2, 21.7, 24.2, 26.8, 29.4 seconds.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 1.3, 3.8, 6.4, 8.9, 11.5, 14.0, 16.6, 19.2, 21.7, 24.2, 26.8, 29.4 seconds.\n",
      "###Human: What is the woman doing in the middle of the video?\n",
      "###Assistant:\n",
      "The woman in the middle of the video is using a laptop computer.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 1.3, 3.8, 6.4, 8.9, 11.5, 14.0, 16.6, 19.2, 21.7, 24.2, 26.8, 29.4 seconds.\n",
      "###Human: What is the woman doing in the middle of the video?\n",
      "###Assistant: The woman in the middle of the video is using a laptop computer.\n",
      "###Human: And at the end of the video?\n",
      "###Assistant:\n",
      "At the end of the video, a woman is opening a door with a key.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 1.3, 3.8, 6.4, 8.9, 11.5, 14.0, 16.6, 19.2, 21.7, 24.2, 26.8, 29.4 seconds.\n",
      "###Human: What is the woman doing in the middle of the video?\n",
      "###Assistant: The woman in the middle of the video is using a laptop computer.\n",
      "###Human: And at the end of the video?\n",
      "###Assistant: At the end of the video, a woman is opening a door with a key.\n",
      "###Human: And at the beginning of the video?\n",
      "###Assistant:\n",
      "At the beginning of the video, a woman is standing in front of a mirror.\n"
     ]
    }
   ],
   "source": [
    "vid_path = \"data/videos/charades/00NN7.mp4\"\n",
    "instruction = \"Watch the video and choose the most fitting option based on the observed content.\"\n",
    "img_list, msg = encode_video(model, vid_path, instruction)\n",
    "\n",
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([\"\", instruction])\n",
    "chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> %s\\n\" % msg])\n",
    "\n",
    "ask(\"What is the woman doing in the middle of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"And at the end of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"And at the beginning of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_position: 2352\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 12\n",
      "Interpolate the position embedding\n",
      "The video contains 12 frames sampled at 5.4, 16.3, 27.3, 38.2, 49.1, 60.0, 70.9, 81.8, 92.7, 103.6, 114.5, 125.4 seconds.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 5.4, 16.3, 27.3, 38.2, 49.1, 60.0, 70.9, 81.8, 92.7, 103.6, 114.5, 125.4 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is about a weather forecast for the next few days.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 5.4, 16.3, 27.3, 38.2, 49.1, 60.0, 70.9, 81.8, 92.7, 103.6, 114.5, 125.4 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant: The video is about a weather forecast for the next few days.\n",
      "###Human: What will the weather be like according to the video?\n",
      "###Assistant:\n",
      "The video shows a forecast for the next few days, including a tropical storm and a heavy rain event.\n",
      "###: Watch the video and choose the most fitting option based on the observed content.###Human: <Video><VideoHere></Video> The video contains 12 frames sampled at 5.4, 16.3, 27.3, 38.2, 49.1, 60.0, 70.9, 81.8, 92.7, 103.6, 114.5, 125.4 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant: The video is about a weather forecast for the next few days.\n",
      "###Human: What will the weather be like according to the video?\n",
      "###Assistant: The video shows a forecast for the next few days, including a tropical storm and a heavy rain event.\n",
      "###Human: At which segment of the video is the temperature shown, and how will it be?\n",
      "###Assistant:\n",
      "The temperature is shown at the end of the video, and it is expected to be around 70 degrees.\n"
     ]
    }
   ],
   "source": [
    "vid_path = 'data/videos/qvhighlights/_-5vXZfppKE_60.0_210.0.mp4'\n",
    "instruction = \"Watch the video and choose the most fitting option based on the observed content.\"\n",
    "img_list, msg = encode_video(model, vid_path, instruction)\n",
    "\n",
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([\"\", instruction])\n",
    "chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> %s\\n\" % msg])\n",
    "\n",
    "ask(\"What is this video about?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"What will the weather be like according to the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"At which segment of the video is the temperature shown, and how will it be?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_position: 3920\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 20\n",
      "Interpolate the position embedding\n",
      "The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###: Examine the video and make a decision based on the content presented in the footage.###Human: <Video><VideoHere></Video> The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video is about a girl who is using a laptop and a phone to make a video call.\n",
      "###: Examine the video and make a decision based on the content presented in the footage.###Human: <Video><VideoHere></Video> The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant: The video is about a girl who is using a laptop and a phone to make a video call.\n",
      "###Human: What happens at the beginning of the video?\n",
      "###Assistant:\n",
      "At the beginning of the video, a girl is shown with a cat on her phone.\n",
      "###: Examine the video and make a decision based on the content presented in the footage.###Human: <Video><VideoHere></Video> The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###Human: What is this video about?\n",
      "###Assistant: The video is about a girl who is using a laptop and a phone to make a video call.\n",
      "###Human: What happens at the beginning of the video?\n",
      "###Assistant: At the beginning of the video, a girl is shown with a cat on her phone.\n",
      "###Human: In which part of the video does a pot of noodles appear\n",
      "###Assistant:\n",
      "A pot of noodles appears in the video at the end.\n"
     ]
    }
   ],
   "source": [
    "vid_path = 'data/videos/qvhighlights/_2mgEMfnYzw_60.0_210.0.mp4'\n",
    "instruction = \"Examine the video and make a decision based on the content presented in the footage.\"\n",
    "img_list, msg = encode_video(model, vid_path, instruction, num_frame=20)\n",
    "\n",
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([\"\", instruction])\n",
    "chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> %s\\n\" % msg])\n",
    "\n",
    "ask(\"What is this video about?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"What happens at the beginning of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"In which part of the video does a pot of noodles appear?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_position: 3920\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 20\n",
      "Interpolate the position embedding\n",
      "The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###: Examine the video and make a decision based on the content presented in the footage.###Human: <Video><VideoHere></Video> The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###Human: During which part of the video does 'the woman shows different dishes' occur, at the beginning, the middle or the end of the video?\n",
      "###Assistant:\n",
      "The woman shows different dishes at the beginning of the video.\n",
      "###: Examine the video and make a decision based on the content presented in the footage.###Human: <Video><VideoHere></Video> The video contains 20 frames sampled at 3.7, 11.2, 18.7, 26.2, 33.7, 41.2, 48.7, 56.2, 63.7, 71.2, 78.7, 86.2, 93.7, 101.2, 108.7, 116.2, 123.7, 131.2, 138.7, 146.2 seconds.\n",
      "###Human: During which part of the video does 'the woman shows different dishes' occur, at the beginning, the middle or the end of the video?\n",
      "###Assistant: The woman shows different dishes at the beginning of the video.\n",
      "###Human: What happens at the end of the video?\n",
      "###Assistant:\n",
      "At the end of the video, a person is shown cutting a piece of food on a plate.\n"
     ]
    }
   ],
   "source": [
    "vid_path = 'data/videos/qvhighlights/_4tpo4pUN3k_510.0_660.0.mp4'\n",
    "instruction = \"Examine the video and make a decision based on the content presented in the footage.\"\n",
    "img_list, msg = encode_video(model, vid_path, instruction, num_frame=20)\n",
    "\n",
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([\"\", instruction])\n",
    "chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> %s\\n\" % msg])\n",
    "\n",
    "ask(\"During which part of the video does 'the woman shows different dishes' occur, at the beginning, the middle or the end of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n",
    "\n",
    "ask(\"What happens at the end of the video?\", chat)\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True)[0]\n",
    "print(llm_message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
